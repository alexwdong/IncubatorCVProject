{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TrainDatasetWithFeat.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws41ZKV6P-e1",
        "colab_type": "text"
      },
      "source": [
        "## Set project directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjscxaYWPmUz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "629f14d0-ffb1-405e-9a5b-a1bd4d28e7fd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnGuSukAP5TE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af63175c-1bed-4880-a697-b2a6cc03831e"
      },
      "source": [
        "%cd drive/My\\ Drive/CV_incubator/IncubatorCVProject"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/CV_incubator/IncubatorCVProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNlgaz2qQJFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "db7e7b87-1432-43d1-cb8b-721c9fa491c0"
      },
      "source": [
        "!ls src"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Basic_CNN_Architecture.py   dataset.py\t\t    __pycache__\n",
            "BasicCNN_withfeat_64x64.py  feature_engineering.py  train_valid.py\n",
            "data_loader.py\t\t    main_two.py\t\t    utils.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpqkhJuDQSGB",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcpZjgXAQmF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from torchvision.transforms import ToTensor,Resize,Compose,Grayscale\n",
        "from src.data_loader import SquarePadding\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch\n",
        "import pickle\n",
        "import time\n",
        "from src.BasicCNN_withfeat_64x64 import BasicCNN_w_features_64x64 \n",
        "from src.train_valid import train, validation\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCggsPKlQT8w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Path to the folder where all images are stored\n",
        "# data_path = '../indoor'\n",
        "data_path = '../dog-breed-identification'"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVcfh38Rrsx3",
        "colab_type": "text"
      },
      "source": [
        "#### Pytorch dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9WuvDYYFkXQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = ImageFolder(data_path,transform=Compose([\n",
        "    SquarePadding(),\n",
        "    Resize((64,64)),\n",
        "    ToTensor()]))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geCCUqrch34d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "f3bd62d7-31b2-49ae-a42f-893251501b95"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "classes = [root.split('/')[-1] for root,dir,files in os.walk(data_path)][1:]\n",
        "class_count = [len(files) for root,dir,files in os.walk(data_path)][1:]\n",
        "cls = list(zip(classes,class_count))\n",
        "cls = sorted(cls,key = lambda cls: cls[1], reverse=True)\n",
        "plt.bar([x[0] for x in cls],[x[1] for x in cls])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 120 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3-cSxK9rC0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Variables for splitting the dataset into train/test\n",
        "validation_split = .2\n",
        "test_split = .1\n",
        "batch_size = 16\n",
        "shuffle_dataset = True\n",
        "random_seed = 42\n",
        "\n",
        "# Split \n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split_idx1 = int(np.floor((validation_split+test_split) * dataset_size))\n",
        "split_idx2 = int(np.floor(test_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "test_indices, val_indices, train_indices = indices[:split_idx2], indices[split_idx2:split_idx1], indices[split_idx1:]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9y1_iOwhrxtN",
        "colab_type": "text"
      },
      "source": [
        "#### Engineered features(eigenvectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLf4mqo-ozLC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_feat = ImageFolder(data_path,transform=Compose([\n",
        "    SquarePadding(),\n",
        "    Resize((64,64)),\n",
        "    Grayscale(num_output_channels=1),\n",
        "    ToTensor()]))\n",
        "\n",
        "train_loader_feat = torch.utils.data.DataLoader(dataset_feat, batch_size=len(train_indices), \n",
        "                                           sampler=train_sampler)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk36bFxWWDLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = next(iter(train_loader_feat))\n",
        "# pickle.dump(sample[0], open( \"imagelist_indoor_train.p\", \"wb\" ) )"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFAyDhYsD9qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_list = sample[0]"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPmAqYEFp3lw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(\"imagelist_indoor_train.p\", \"rb\" )\n",
        "image_list = pickle.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTlNCtf110wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from src.utils import unravel_image, ravel_image_vec, plot_image_grid"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcGQoBNUMeUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PCA_images_list(images_list,n_components=100):\n",
        "    #Initialize X\n",
        "    img_h,img_w = images_list.size()[-2:]\n",
        "    default_image_size = images_list.size()[-2:]\n",
        "    num_pixels = img_h*img_w\n",
        "    X = images_list.flatten(1).numpy()\n",
        "    #Now grab all the images and push them into X\n",
        "\n",
        "    #Perform PCA on X\n",
        "    PCA_model = PCA(n_components=n_components)\n",
        "    PCA_model.fit(X)\n",
        "    eig_vecs =PCA_model.components_\n",
        "    eig_vals=PCA_model.explained_variance_ \n",
        "    #NOTE!!! The return of this is of shape (n_components,length_of_eig_vec).\n",
        "    #Each row is an eigenvectors, and the first row is the eigenvector with the\n",
        "    #largest eigenvalue\n",
        "   \n",
        "    #\"ravel\" each eigenvector and push into new_eigvec_list\n",
        "    # new_eigvec = torch.Tensor(eig_vecs).view((n_components,img_h,img_w))\n",
        "    new_eigvec = torch.Tensor(eig_vecs)\n",
        "    #Now, return the eigval-eigvec pairs\n",
        "    return (eig_vals,new_eigvec)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCbw7hblMSy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(eig_vals,eig_image_list) = PCA_images_list(image_list)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPKtZS9vviUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_feat = valid_feat = eig_image_list"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbAjSuRstpi0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_eigen_component_features(images_list,eig_vecs):\n",
        "    '''\n",
        "    Inputs:\n",
        "        images_list: list of images (images are ndarrays, usually of 2 dimensions (e.g 128x128))\n",
        "        eig_vecs: an 2d array of eigenvectors, where each eigenvector is a column\n",
        "    Outputs:\n",
        "        return: returns a feature matrix, where the rows of the feature matrix correspond to the \n",
        "        features of images. First image is the first row of the matrix, last image is the last row.\n",
        "        The features are the components of the eigenvectors (which were passed in the input 'eig_vecs')\n",
        "    '''\n",
        "    #n_components = len(eig_vecs.shape)\n",
        "    feat = torch.transpose(eig_vecs,0,1)\n",
        "    image_vec = torch.flatten(images_list,1)\n",
        "    feature_matrix = image_vec@feat\n",
        "        \n",
        "    return feature_matrix"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQHumkInsIH7",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txmSk8M8xA9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from skimage import io, color\n",
        "\n",
        "import time\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train(epoch, model, optimizer, criterion, loader, device, train_feat):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    mysoftmax = nn.Softmax(dim=1)\n",
        "\n",
        "    for batch_idx, (image, label) in enumerate(loader):\n",
        "        \n",
        "        \n",
        "        batch_size = image.size(0)\n",
        "        gray_image = torch.stack([transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.ToTensor()])(image[i]) \n",
        "            for i in range(image.size(0))])\n",
        "        \n",
        "        image = image.to(device)\n",
        "       \n",
        "        #label = samples[1].squeeze().to(device)\n",
        "        label = label.to(device)\n",
        "        \n",
        "        #label = torch.tensor(label, dtype=torch.long, device=device)\n",
        "        #print(image)\n",
        "        \n",
        "        feat = prepare_eigen_component_features(gray_image,train_feat).to(device)\n",
        "        output = model(image,feat)\n",
        "        _, preds = torch.max(output, dim = 1)\n",
        "    \n",
        "        loss = criterion(output, label)\n",
        "        running_loss += loss.item()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        total_samples += image.shape[0]\n",
        "        correct += torch.sum(preds == label).item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "    training_accuracy = correct / total_samples\n",
        "\n",
        "    return epoch_loss / len(loader), training_accuracy\n",
        "\n",
        "def validation(epoch, model, optimizer, criterion, loader, device, valid_feat, multiclass=True):\n",
        "\n",
        "    model.eval( )\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total_samples = 0\n",
        "    correct = 0\n",
        "    mysoftmax = nn.Softmax(dim=1)\n",
        "\n",
        "    # this part needs to be adapted later based on task 3\n",
        "    preds_list = []\n",
        "    truelabels_list = []\n",
        "    probas_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (image, label) in enumerate(loader):\n",
        "            batch_size = image.size(0)\n",
        "            gray_image = torch.stack([transforms.Compose([\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Grayscale(num_output_channels=1),\n",
        "                transforms.ToTensor()])(image[i]) \n",
        "                for i in range(image.size(0))])\n",
        "            \n",
        "            image = image.to(device)\n",
        "          \n",
        "            #label = samples[1].squeeze().to(device)\n",
        "            label = label.to(device)\n",
        "            \n",
        "            #label = torch.tensor(label, dtype=torch.long, device=device)\n",
        "            #print(image)\n",
        "            \n",
        "            feat = prepare_eigen_component_features(gray_image,valid_feat).to(device)\n",
        "            output = model(image,feat)\n",
        "            output_softmax = mysoftmax(output)\n",
        "\n",
        "            _, preds = torch.max(output, dim = 1)\n",
        "\n",
        "            loss = criterion(output, label)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            total_samples += image.shape[0]\n",
        "            correct += torch.sum(preds == label).item()\n",
        "\n",
        "            preds_list.append(preds.cpu().numpy())\n",
        "            truelabels_list.append(label.cpu().numpy())\n",
        "            probas_list.append(output_softmax.cpu().numpy())\n",
        "\n",
        "        valid_accuracy = correct / total_samples\n",
        "\n",
        "        probas_list = np.vstack(probas_list)\n",
        "        truelabels_list = np.concatenate(truelabels_list)\n",
        "        preds_list = np.concatenate(preds_list)\n",
        "\n",
        "        if multiclass == False:\n",
        "            auc_score = metrics.roc_auc_score(truelabels_list, preds_list)\n",
        "\n",
        "        else:\n",
        "            # Computes the average AUC of all possible pairwise combinations of classes\n",
        "            # Insensitive to class imbalance when average == 'macro'\n",
        "            auc_score = metrics.roc_auc_score(truelabels_list, probas_list, multi_class='ovo')\n",
        "\n",
        "\n",
        "        return running_loss / len(loader), valid_accuracy, preds_list, truelabels_list, probas_list, auc_score"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N13C8JmHsJiQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "07d37bf8-ed59-43a2-b051-07877f484f1c"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "    \n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV_5smrRtFlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicCNN_w_features_64x64(nn.Module):\n",
        "    def __init__(self,num_classes):\n",
        "        super(BasicCNN_w_features_64x64, self).__init__()\n",
        "\n",
        "        in_channels= 3  \n",
        "        conv1_out_channels= 8 \n",
        "        conv2_out_channels = 8\n",
        "        lin1_out_channels= 64\n",
        "        kernel_size = 5\n",
        "        stride = 2\n",
        "        padding = 2\n",
        "        dropout = None\n",
        "        activation = nn.ReLU(inplace = False)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels, \n",
        "            out_channels=conv1_out_channels,\n",
        "            kernel_size=kernel_size, \n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            padding_mode='zeros'\n",
        "            )\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=conv1_out_channels, \n",
        "            out_channels=conv2_out_channels,\n",
        "            kernel_size=kernel_size, \n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            padding_mode='zeros'\n",
        "            )\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(in_features=4*4*conv2_out_channels + 100,\n",
        "                             out_features=lin1_out_channels)\n",
        "        # infeat_size = (featmap_size + num_img * num_eigenvecs)\n",
        "        self.fc2 = nn.Linear(in_features=lin1_out_channels,\n",
        "                             out_features=num_classes)\n",
        "        \n",
        "        if dropout is not None:\n",
        "            if (dropout > 1 or dropout < 0) or type(dropout) is not float:\n",
        "                raise ValueError(\"Give Dropout Probability between 0 and 1\")    \n",
        "            else:\n",
        "                self.dropout1 = nn.Dropout(p = dropout, inplace = False)\n",
        "                self.dropout2 = nn.Dropout(p = dropout, inplace = False)\n",
        "        else:\n",
        "            self.dropout1 = None\n",
        "            self.dropout2 = None\n",
        "        \n",
        "        self.activation = activation\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "    def forward(self,image,features):\n",
        "        \n",
        "        out = self.conv1(image)\n",
        "        #print(1,out.size())\n",
        "        out = self.activation(out)\n",
        "        #print(2,out.size())\n",
        "        out = self.pool1(out)\n",
        "        #print(3,out.size())\n",
        "        if self.dropout1 is not None:\n",
        "            out = self.dropout2(out) \n",
        "        out = self.conv2(out)\n",
        "        #print(4,out.size())\n",
        "        out = self.activation(out)\n",
        "        #print(5,out.size())\n",
        "        out = self.pool2(out)\n",
        "        #print(6,out.size())\n",
        "        if self.dropout2 is not None:\n",
        "            out = self.dropout2(out)\n",
        "        out = self.flatten(out)\n",
        "        out = torch.cat((out,features),1)\n",
        "        out = self.fc1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.softmax(out)\n",
        "        return out"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZekF-KjsNRk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "af94c5f7-f5a9-4d0c-8ecb-81c906e9eeef"
      },
      "source": [
        "num_classes=len(dataset.classes)\n",
        "\n",
        "# training process\n",
        "# to be finished later\n",
        "model = BasicCNN_w_features_64x64(num_classes=num_classes)\n",
        "model.to(device)\n",
        "train_loss = nn.CrossEntropyLoss()\n",
        "train_loss.to(device)\n",
        "valid_loss = nn.CrossEntropyLoss()\n",
        "valid_loss.to(device)\n",
        "def train_valid(optimizer = optim.Adam(model.parameters()), epochs = 20, model = model,\n",
        "                train_criterion = train_loss, train_loader = train_loader,\n",
        "                valid_criterion = valid_loss, valid_loader = valid_loader,\n",
        "                device = device, train_feat = train_feat, valid_feat = valid_feat):\n",
        "\n",
        "    start_epoch = 1\n",
        "    #or: best_val_acc = 0\n",
        "    best_val_loss = np.inf\n",
        "\n",
        "    history = {\"train_loss\":[], \"train_acc\":[],\n",
        "                \"valid_loss\":[], \"valid_acc\":[], \"valid_preds_list\":[],\n",
        "                \"valid_truelabels_list\":[], \"valid_probas_list\":[], \"valid_auc_score\":[]}\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(start_epoch, epochs + 1):\n",
        "\n",
        "        train_loss, train_acc = train(epoch, model, optimizer, train_criterion, \n",
        "                                      train_loader, device, train_feat)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "\n",
        "        print('epoch: ', epoch)\n",
        "        print('{}: loss: {:.4f} acc: {:.4f}'.format('training', train_loss, train_acc))\n",
        "\n",
        "        valid_loss, valid_acc, valid_preds_list, valid_truelabels_list, valid_probas_list, valid_auc_score = validation(epoch, model, optimizer, \n",
        "                                                        valid_criterion, valid_loader, \n",
        "                                                        device, valid_feat)\n",
        "        history[\"valid_loss\"].append(valid_loss)\n",
        "        history[\"valid_acc\"].append(valid_acc)\n",
        "        history[\"valid_preds_list\"].append(valid_preds_list)\n",
        "        history[\"valid_truelabels_list\"].append(valid_truelabels_list)\n",
        "        history[\"valid_probas_list\"].append(valid_probas_list)\n",
        "        history[\"valid_auc_score\"].append(valid_auc_score)\n",
        "\n",
        "        print('{}: loss: {:.4f} acc: {:.4f} auc: {:.4f}'.format('validation', valid_loss, valid_acc, valid_auc_score))\n",
        "        print()\n",
        "\n",
        "        # save models(use valid loss as best model criterion, please change\n",
        "        # criterion here if needed(eg. valid acc)\n",
        "        is_best = valid_loss < best_val_loss\n",
        "        best_val_loss = min(valid_loss, best_val_loss)\n",
        "\n",
        "        if is_best:\n",
        "            # please change model file path here\n",
        "            best_model_file = \"best_dry_run1.pth\"\n",
        "            torch.save(model.state_dict(), best_model_file)\n",
        "\n",
        "        # save model from every training epoch\n",
        "        # can be deleted if do not need this one, or adapt it to save 5th, 10th, 15th ...models\n",
        "        model_file = \"dry_run1\" + str(epoch) + \".pth\"\n",
        "\n",
        "        torch.save(model.state_dict(), model_file)\n",
        "\n",
        "        # save training/validation results\n",
        "        with open(\"history.pkl\", \"wb\") as fout:\n",
        "            pickle.dump(history, fout)\n",
        "\n",
        "    print('time elapsed:', time.time() - start_time)\n",
        "\n",
        "    return history\n",
        "\n",
        "results = train_valid(epochs = 100)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch:  1\n",
            "training: loss: 4.7802 acc: 0.0194\n",
            "validation: loss: 4.7802 acc: 0.0199 auc: 0.5187\n",
            "\n",
            "epoch:  2\n",
            "training: loss: 4.7707 acc: 0.0298\n",
            "validation: loss: 4.7723 acc: 0.0269 auc: 0.5359\n",
            "\n",
            "epoch:  3\n",
            "training: loss: 4.7631 acc: 0.0384\n",
            "validation: loss: 4.7713 acc: 0.0284 auc: 0.5392\n",
            "\n",
            "epoch:  4\n",
            "training: loss: 4.7541 acc: 0.0488\n",
            "validation: loss: 4.7664 acc: 0.0361 auc: 0.5377\n",
            "\n",
            "epoch:  5\n",
            "training: loss: 4.7459 acc: 0.0581\n",
            "validation: loss: 4.7654 acc: 0.0364 auc: 0.5423\n",
            "\n",
            "epoch:  6\n",
            "training: loss: 4.7378 acc: 0.0663\n",
            "validation: loss: 4.7635 acc: 0.0361 auc: 0.5413\n",
            "\n",
            "epoch:  7\n",
            "training: loss: 4.7316 acc: 0.0731\n",
            "validation: loss: 4.7581 acc: 0.0456 auc: 0.5422\n",
            "\n",
            "epoch:  8\n",
            "training: loss: 4.7240 acc: 0.0809\n",
            "validation: loss: 4.7593 acc: 0.0419 auc: 0.5393\n",
            "\n",
            "epoch:  9\n",
            "training: loss: 4.7185 acc: 0.0869\n",
            "validation: loss: 4.7562 acc: 0.0456 auc: 0.5431\n",
            "\n",
            "epoch:  10\n",
            "training: loss: 4.7123 acc: 0.0926\n",
            "validation: loss: 4.7575 acc: 0.0429 auc: 0.5420\n",
            "\n",
            "epoch:  11\n",
            "training: loss: 4.7066 acc: 0.0990\n",
            "validation: loss: 4.7549 acc: 0.0465 auc: 0.5421\n",
            "\n",
            "epoch:  12\n",
            "training: loss: 4.7006 acc: 0.1062\n",
            "validation: loss: 4.7559 acc: 0.0465 auc: 0.5449\n",
            "\n",
            "epoch:  13\n",
            "training: loss: 4.6945 acc: 0.1105\n",
            "validation: loss: 4.7563 acc: 0.0441 auc: 0.5446\n",
            "\n",
            "epoch:  14\n",
            "training: loss: 4.6899 acc: 0.1159\n",
            "validation: loss: 4.7541 acc: 0.0460 auc: 0.5434\n",
            "\n",
            "epoch:  15\n",
            "training: loss: 4.6857 acc: 0.1196\n",
            "validation: loss: 4.7549 acc: 0.0448 auc: 0.5437\n",
            "\n",
            "epoch:  16\n",
            "training: loss: 4.6816 acc: 0.1236\n",
            "validation: loss: 4.7561 acc: 0.0444 auc: 0.5452\n",
            "\n",
            "epoch:  17\n",
            "training: loss: 4.6782 acc: 0.1265\n",
            "validation: loss: 4.7529 acc: 0.0468 auc: 0.5481\n",
            "\n",
            "epoch:  18\n",
            "training: loss: 4.6736 acc: 0.1318\n",
            "validation: loss: 4.7530 acc: 0.0473 auc: 0.5454\n",
            "\n",
            "epoch:  19\n",
            "training: loss: 4.6685 acc: 0.1368\n",
            "validation: loss: 4.7526 acc: 0.0480 auc: 0.5432\n",
            "\n",
            "epoch:  20\n",
            "training: loss: 4.6658 acc: 0.1393\n",
            "validation: loss: 4.7552 acc: 0.0451 auc: 0.5434\n",
            "\n",
            "epoch:  21\n",
            "training: loss: 4.6628 acc: 0.1419\n",
            "validation: loss: 4.7523 acc: 0.0487 auc: 0.5447\n",
            "\n",
            "epoch:  22\n",
            "training: loss: 4.6584 acc: 0.1461\n",
            "validation: loss: 4.7535 acc: 0.0482 auc: 0.5407\n",
            "\n",
            "epoch:  23\n",
            "training: loss: 4.6561 acc: 0.1492\n",
            "validation: loss: 4.7528 acc: 0.0480 auc: 0.5416\n",
            "\n",
            "epoch:  24\n",
            "training: loss: 4.6522 acc: 0.1528\n",
            "validation: loss: 4.7539 acc: 0.0465 auc: 0.5426\n",
            "\n",
            "epoch:  25\n",
            "training: loss: 4.6490 acc: 0.1555\n",
            "validation: loss: 4.7518 acc: 0.0477 auc: 0.5427\n",
            "\n",
            "epoch:  26\n",
            "training: loss: 4.6463 acc: 0.1580\n",
            "validation: loss: 4.7512 acc: 0.0480 auc: 0.5418\n",
            "\n",
            "epoch:  27\n",
            "training: loss: 4.6442 acc: 0.1609\n",
            "validation: loss: 4.7494 acc: 0.0509 auc: 0.5430\n",
            "\n",
            "epoch:  28\n",
            "training: loss: 4.6415 acc: 0.1629\n",
            "validation: loss: 4.7527 acc: 0.0475 auc: 0.5431\n",
            "\n",
            "epoch:  29\n",
            "training: loss: 4.6397 acc: 0.1647\n",
            "validation: loss: 4.7516 acc: 0.0482 auc: 0.5421\n",
            "\n",
            "epoch:  30\n",
            "training: loss: 4.6363 acc: 0.1677\n",
            "validation: loss: 4.7531 acc: 0.0468 auc: 0.5414\n",
            "\n",
            "epoch:  31\n",
            "training: loss: 4.6337 acc: 0.1706\n",
            "validation: loss: 4.7525 acc: 0.0480 auc: 0.5423\n",
            "\n",
            "epoch:  32\n",
            "training: loss: 4.6322 acc: 0.1715\n",
            "validation: loss: 4.7520 acc: 0.0490 auc: 0.5392\n",
            "\n",
            "epoch:  33\n",
            "training: loss: 4.6305 acc: 0.1736\n",
            "validation: loss: 4.7512 acc: 0.0497 auc: 0.5413\n",
            "\n",
            "epoch:  34\n",
            "training: loss: 4.6279 acc: 0.1751\n",
            "validation: loss: 4.7504 acc: 0.0509 auc: 0.5407\n",
            "\n",
            "epoch:  35\n",
            "training: loss: 4.6261 acc: 0.1773\n",
            "validation: loss: 4.7495 acc: 0.0524 auc: 0.5407\n",
            "\n",
            "epoch:  36\n",
            "training: loss: 4.6258 acc: 0.1772\n",
            "validation: loss: 4.7518 acc: 0.0475 auc: 0.5405\n",
            "\n",
            "epoch:  37\n",
            "training: loss: 4.6262 acc: 0.1780\n",
            "validation: loss: 4.7508 acc: 0.0490 auc: 0.5381\n",
            "\n",
            "epoch:  38\n",
            "training: loss: 4.6225 acc: 0.1809\n",
            "validation: loss: 4.7512 acc: 0.0485 auc: 0.5409\n",
            "\n",
            "epoch:  39\n",
            "training: loss: 4.6219 acc: 0.1813\n",
            "validation: loss: 4.7512 acc: 0.0502 auc: 0.5382\n",
            "\n",
            "epoch:  40\n",
            "training: loss: 4.6192 acc: 0.1836\n",
            "validation: loss: 4.7525 acc: 0.0497 auc: 0.5376\n",
            "\n",
            "epoch:  41\n",
            "training: loss: 4.6177 acc: 0.1855\n",
            "validation: loss: 4.7512 acc: 0.0497 auc: 0.5381\n",
            "\n",
            "epoch:  42\n",
            "training: loss: 4.6165 acc: 0.1870\n",
            "validation: loss: 4.7521 acc: 0.0487 auc: 0.5384\n",
            "\n",
            "epoch:  43\n",
            "training: loss: 4.6154 acc: 0.1877\n",
            "validation: loss: 4.7490 acc: 0.0528 auc: 0.5385\n",
            "\n",
            "epoch:  44\n",
            "training: loss: 4.6149 acc: 0.1881\n",
            "validation: loss: 4.7518 acc: 0.0473 auc: 0.5371\n",
            "\n",
            "epoch:  45\n",
            "training: loss: 4.6155 acc: 0.1876\n",
            "validation: loss: 4.7505 acc: 0.0502 auc: 0.5401\n",
            "\n",
            "epoch:  46\n",
            "training: loss: 4.6144 acc: 0.1888\n",
            "validation: loss: 4.7487 acc: 0.0524 auc: 0.5405\n",
            "\n",
            "epoch:  47\n",
            "training: loss: 4.6112 acc: 0.1914\n",
            "validation: loss: 4.7504 acc: 0.0516 auc: 0.5386\n",
            "\n",
            "epoch:  48\n",
            "training: loss: 4.6103 acc: 0.1924\n",
            "validation: loss: 4.7482 acc: 0.0531 auc: 0.5376\n",
            "\n",
            "epoch:  49\n",
            "training: loss: 4.6110 acc: 0.1917\n",
            "validation: loss: 4.7503 acc: 0.0514 auc: 0.5372\n",
            "\n",
            "epoch:  50\n",
            "training: loss: 4.6093 acc: 0.1932\n",
            "validation: loss: 4.7509 acc: 0.0504 auc: 0.5379\n",
            "\n",
            "epoch:  51\n",
            "training: loss: 4.6098 acc: 0.1929\n",
            "validation: loss: 4.7524 acc: 0.0477 auc: 0.5375\n",
            "\n",
            "epoch:  52\n",
            "training: loss: 4.6092 acc: 0.1941\n",
            "validation: loss: 4.7496 acc: 0.0511 auc: 0.5388\n",
            "\n",
            "epoch:  53\n",
            "training: loss: 4.6067 acc: 0.1957\n",
            "validation: loss: 4.7495 acc: 0.0504 auc: 0.5374\n",
            "\n",
            "epoch:  54\n",
            "training: loss: 4.6051 acc: 0.1974\n",
            "validation: loss: 4.7512 acc: 0.0480 auc: 0.5367\n",
            "\n",
            "epoch:  55\n",
            "training: loss: 4.6045 acc: 0.1981\n",
            "validation: loss: 4.7481 acc: 0.0536 auc: 0.5374\n",
            "\n",
            "epoch:  56\n",
            "training: loss: 4.6041 acc: 0.1986\n",
            "validation: loss: 4.7497 acc: 0.0511 auc: 0.5380\n",
            "\n",
            "epoch:  57\n",
            "training: loss: 4.6043 acc: 0.1982\n",
            "validation: loss: 4.7536 acc: 0.0460 auc: 0.5368\n",
            "\n",
            "epoch:  58\n",
            "training: loss: 4.6026 acc: 0.1999\n",
            "validation: loss: 4.7486 acc: 0.0533 auc: 0.5354\n",
            "\n",
            "epoch:  59\n",
            "training: loss: 4.6026 acc: 0.1999\n",
            "validation: loss: 4.7490 acc: 0.0511 auc: 0.5373\n",
            "\n",
            "epoch:  60\n",
            "training: loss: 4.6018 acc: 0.2007\n",
            "validation: loss: 4.7521 acc: 0.0482 auc: 0.5372\n",
            "\n",
            "epoch:  61\n",
            "training: loss: 4.6001 acc: 0.2019\n",
            "validation: loss: 4.7517 acc: 0.0499 auc: 0.5345\n",
            "\n",
            "epoch:  62\n",
            "training: loss: 4.5995 acc: 0.2029\n",
            "validation: loss: 4.7531 acc: 0.0470 auc: 0.5376\n",
            "\n",
            "epoch:  63\n",
            "training: loss: 4.6014 acc: 0.2010\n",
            "validation: loss: 4.7510 acc: 0.0499 auc: 0.5363\n",
            "\n",
            "epoch:  64\n",
            "training: loss: 4.5989 acc: 0.2032\n",
            "validation: loss: 4.7514 acc: 0.0502 auc: 0.5371\n",
            "\n",
            "epoch:  65\n",
            "training: loss: 4.5996 acc: 0.2025\n",
            "validation: loss: 4.7503 acc: 0.0507 auc: 0.5363\n",
            "\n",
            "epoch:  66\n",
            "training: loss: 4.5981 acc: 0.2039\n",
            "validation: loss: 4.7510 acc: 0.0502 auc: 0.5353\n",
            "\n",
            "epoch:  67\n",
            "training: loss: 4.5993 acc: 0.2029\n",
            "validation: loss: 4.7520 acc: 0.0492 auc: 0.5341\n",
            "\n",
            "epoch:  68\n",
            "training: loss: 4.5982 acc: 0.2045\n",
            "validation: loss: 4.7507 acc: 0.0492 auc: 0.5367\n",
            "\n",
            "epoch:  69\n",
            "training: loss: 4.5963 acc: 0.2055\n",
            "validation: loss: 4.7514 acc: 0.0494 auc: 0.5359\n",
            "\n",
            "epoch:  70\n",
            "training: loss: 4.5981 acc: 0.2046\n",
            "validation: loss: 4.7533 acc: 0.0465 auc: 0.5351\n",
            "\n",
            "epoch:  71\n",
            "training: loss: 4.5976 acc: 0.2043\n",
            "validation: loss: 4.7508 acc: 0.0494 auc: 0.5359\n",
            "\n",
            "epoch:  72\n",
            "training: loss: 4.5965 acc: 0.2057\n",
            "validation: loss: 4.7501 acc: 0.0507 auc: 0.5364\n",
            "\n",
            "epoch:  73\n",
            "training: loss: 4.5961 acc: 0.2062\n",
            "validation: loss: 4.7487 acc: 0.0531 auc: 0.5360\n",
            "\n",
            "epoch:  74\n",
            "training: loss: 4.5946 acc: 0.2074\n",
            "validation: loss: 4.7526 acc: 0.0487 auc: 0.5350\n",
            "\n",
            "epoch:  75\n",
            "training: loss: 4.5948 acc: 0.2071\n",
            "validation: loss: 4.7508 acc: 0.0492 auc: 0.5344\n",
            "\n",
            "epoch:  76\n",
            "training: loss: 4.5959 acc: 0.2064\n",
            "validation: loss: 4.7524 acc: 0.0492 auc: 0.5353\n",
            "\n",
            "epoch:  77\n",
            "training: loss: 4.5945 acc: 0.2071\n",
            "validation: loss: 4.7530 acc: 0.0470 auc: 0.5329\n",
            "\n",
            "epoch:  78\n",
            "training: loss: 4.5931 acc: 0.2088\n",
            "validation: loss: 4.7522 acc: 0.0494 auc: 0.5343\n",
            "\n",
            "epoch:  79\n",
            "training: loss: 4.5951 acc: 0.2074\n",
            "validation: loss: 4.7532 acc: 0.0473 auc: 0.5332\n",
            "\n",
            "epoch:  80\n",
            "training: loss: 4.5938 acc: 0.2083\n",
            "validation: loss: 4.7523 acc: 0.0494 auc: 0.5334\n",
            "\n",
            "epoch:  81\n",
            "training: loss: 4.5935 acc: 0.2088\n",
            "validation: loss: 4.7550 acc: 0.0458 auc: 0.5348\n",
            "\n",
            "epoch:  82\n",
            "training: loss: 4.5932 acc: 0.2088\n",
            "validation: loss: 4.7506 acc: 0.0511 auc: 0.5333\n",
            "\n",
            "epoch:  83\n",
            "training: loss: 4.5925 acc: 0.2090\n",
            "validation: loss: 4.7532 acc: 0.0475 auc: 0.5332\n",
            "\n",
            "epoch:  84\n",
            "training: loss: 4.5915 acc: 0.2105\n",
            "validation: loss: 4.7522 acc: 0.0487 auc: 0.5338\n",
            "\n",
            "epoch:  85\n",
            "training: loss: 4.5923 acc: 0.2100\n",
            "validation: loss: 4.7534 acc: 0.0480 auc: 0.5329\n",
            "\n",
            "epoch:  86\n",
            "training: loss: 4.5911 acc: 0.2110\n",
            "validation: loss: 4.7527 acc: 0.0475 auc: 0.5335\n",
            "\n",
            "epoch:  87\n",
            "training: loss: 4.5905 acc: 0.2115\n",
            "validation: loss: 4.7547 acc: 0.0460 auc: 0.5321\n",
            "\n",
            "epoch:  88\n",
            "training: loss: 4.5911 acc: 0.2109\n",
            "validation: loss: 4.7536 acc: 0.0473 auc: 0.5334\n",
            "\n",
            "epoch:  89\n",
            "training: loss: 4.5903 acc: 0.2118\n",
            "validation: loss: 4.7509 acc: 0.0497 auc: 0.5344\n",
            "\n",
            "epoch:  90\n",
            "training: loss: 4.5905 acc: 0.2116\n",
            "validation: loss: 4.7545 acc: 0.0458 auc: 0.5338\n",
            "\n",
            "epoch:  91\n",
            "training: loss: 4.5898 acc: 0.2124\n",
            "validation: loss: 4.7509 acc: 0.0507 auc: 0.5321\n",
            "\n",
            "epoch:  92\n",
            "training: loss: 4.5896 acc: 0.2124\n",
            "validation: loss: 4.7527 acc: 0.0490 auc: 0.5342\n",
            "\n",
            "epoch:  93\n",
            "training: loss: 4.5904 acc: 0.2113\n",
            "validation: loss: 4.7518 acc: 0.0487 auc: 0.5322\n",
            "\n",
            "epoch:  94\n",
            "training: loss: 4.5891 acc: 0.2130\n",
            "validation: loss: 4.7506 acc: 0.0502 auc: 0.5338\n",
            "\n",
            "epoch:  95\n",
            "training: loss: 4.5887 acc: 0.2133\n",
            "validation: loss: 4.7514 acc: 0.0502 auc: 0.5338\n",
            "\n",
            "epoch:  96\n",
            "training: loss: 4.5888 acc: 0.2134\n",
            "validation: loss: 4.7545 acc: 0.0458 auc: 0.5328\n",
            "\n",
            "epoch:  97\n",
            "training: loss: 4.5876 acc: 0.2140\n",
            "validation: loss: 4.7506 acc: 0.0511 auc: 0.5336\n",
            "\n",
            "epoch:  98\n",
            "training: loss: 4.5878 acc: 0.2141\n",
            "validation: loss: 4.7495 acc: 0.0536 auc: 0.5327\n",
            "\n",
            "epoch:  99\n",
            "training: loss: 4.5859 acc: 0.2158\n",
            "validation: loss: 4.7523 acc: 0.0477 auc: 0.5323\n",
            "\n",
            "epoch:  100\n",
            "training: loss: 4.5889 acc: 0.2137\n",
            "validation: loss: 4.7498 acc: 0.0514 auc: 0.5327\n",
            "\n",
            "time elapsed: 14247.689558506012\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}