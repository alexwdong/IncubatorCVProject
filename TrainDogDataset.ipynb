{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############# Notice ##############\n",
    "###################################\n",
    "\n",
    "# Some parts need to be adapted later\n",
    "\n",
    "###################################\n",
    "########## import library #########\n",
    "###################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor,Resize,Compose\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.train_valid import train, validation\n",
    "from src.Basic_CNN_Architecture import BasicCNN\n",
    "from src.data_loader import load_dog_data,SquarePadding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################\n",
    "########### load device ###########\n",
    "###################################\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "############ def path #############\n",
    "###################################\n",
    "\n",
    "# hpc\n",
    "# please change file path here\n",
    "        \n",
    "\n",
    "#val_df_path =\n",
    "#test_df_path =\n",
    "#root_dir =\n",
    "\n",
    "\n",
    "\n",
    "#Put your data path here\n",
    "data_path = r'C:\\Users\\Alexander Dong\\IncubatorCVProject\\DogsDataset\\images\\Images'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###################################\n",
    "############ load data ############\n",
    "###################################\n",
    "\n",
    "# after feature engineering(task 3)\n",
    "\n",
    "#Variables for splitting the dataset into train/test\n",
    "valid_split = .1\n",
    "test_split = .1\n",
    "batch_size = 16\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "#Load initial dataset\n",
    "dog_dataset = ImageFolder(data_path,transform=Compose([\n",
    "    SquarePadding(),\n",
    "    Resize((64,64)),\n",
    "    ToTensor()\n",
    "])) \n",
    "# Split \n",
    "dataset_size = len(dog_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split_idx1 = int(np.floor((valid_split+test_split) * dataset_size))\n",
    "split_idx2 = int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "test_indices, valid_indices, train_indices = indices[:split_idx2], indices[split_idx2:split_idx1], indices[split_idx1:]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "#Load Train, valid, test          \n",
    "train_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "test_loader =  torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size,\n",
    "                                                sampler=test_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###################################\n",
      "############## Train ##############\n",
      "###################################\n",
      "epoch:  1\n",
      "training: loss: 4.7557 acc: 0.0142\n",
      "validation: loss: 4.6487 acc: 0.0199 auc: 0.6559\n",
      "\n",
      "epoch:  2\n",
      "training: loss: 4.5270 acc: 0.0313\n",
      "validation: loss: 4.4751 acc: 0.0340 auc: 0.7173\n",
      "\n",
      "epoch:  3\n",
      "training: loss: 4.3961 acc: 0.0447\n",
      "validation: loss: 4.4184 acc: 0.0389 auc: 0.7376\n",
      "\n",
      "epoch:  4\n",
      "training: loss: 4.3129 acc: 0.0540\n",
      "validation: loss: 4.3671 acc: 0.0481 auc: 0.7471\n",
      "\n",
      "epoch:  5\n",
      "training: loss: 4.2533 acc: 0.0622\n",
      "validation: loss: 4.3317 acc: 0.0534 auc: 0.7541\n",
      "\n",
      "epoch:  6\n",
      "training: loss: 4.2028 acc: 0.0685\n",
      "validation: loss: 4.3199 acc: 0.0530 auc: 0.7561\n",
      "\n",
      "epoch:  7\n",
      "training: loss: 4.1609 acc: 0.0722\n",
      "validation: loss: 4.2955 acc: 0.0627 auc: 0.7616\n",
      "\n",
      "epoch:  8\n",
      "training: loss: 4.1243 acc: 0.0795\n",
      "validation: loss: 4.3107 acc: 0.0539 auc: 0.7609\n",
      "\n",
      "epoch:  9\n",
      "training: loss: 4.0954 acc: 0.0843\n",
      "validation: loss: 4.2809 acc: 0.0583 auc: 0.7657\n",
      "\n",
      "epoch:  10\n",
      "training: loss: 4.0694 acc: 0.0847\n",
      "validation: loss: 4.3001 acc: 0.0632 auc: 0.7631\n",
      "\n",
      "epoch:  11\n",
      "training: loss: 4.0438 acc: 0.0897\n",
      "validation: loss: 4.3086 acc: 0.0675 auc: 0.7633\n",
      "\n",
      "epoch:  12\n",
      "training: loss: 4.0182 acc: 0.0917\n",
      "validation: loss: 4.2942 acc: 0.0719 auc: 0.7661\n",
      "\n",
      "epoch:  13\n",
      "training: loss: 3.9992 acc: 0.0966\n",
      "validation: loss: 4.2980 acc: 0.0641 auc: 0.7691\n",
      "\n",
      "epoch:  14\n",
      "training: loss: 3.9779 acc: 0.0979\n",
      "validation: loss: 4.2778 acc: 0.0666 auc: 0.7713\n",
      "\n",
      "epoch:  15\n",
      "training: loss: 3.9613 acc: 0.1016\n",
      "validation: loss: 4.3185 acc: 0.0627 auc: 0.7685\n",
      "\n",
      "epoch:  16\n",
      "training: loss: 3.9397 acc: 0.1060\n",
      "validation: loss: 4.2943 acc: 0.0661 auc: 0.7718\n",
      "\n",
      "epoch:  17\n",
      "training: loss: 3.9268 acc: 0.1078\n",
      "validation: loss: 4.2893 acc: 0.0709 auc: 0.7729\n",
      "\n",
      "epoch:  18\n",
      "training: loss: 3.9077 acc: 0.1107\n",
      "validation: loss: 4.2953 acc: 0.0739 auc: 0.7723\n",
      "\n",
      "epoch:  19\n",
      "training: loss: 3.8920 acc: 0.1102\n",
      "validation: loss: 4.3119 acc: 0.0709 auc: 0.7707\n",
      "\n",
      "epoch:  20\n",
      "training: loss: 3.8817 acc: 0.1133\n",
      "validation: loss: 4.3684 acc: 0.0617 auc: 0.7634\n",
      "\n",
      "time elapsed: 1955.1991801261902\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################################\n",
    "############ Train ############\n",
    "###################################\n",
    "print('###################################')\n",
    "print('############## Train ##############')\n",
    "print('###################################')\n",
    "\n",
    "\n",
    "# training process\n",
    "# to be finished later\n",
    "model = BasicCNN(in_channels=3, #RGB\n",
    "\t\t\t     enc_channels=8,\n",
    "\t\t\t     out_channels=12,\n",
    "\t\t\t     lin_channels=32,\n",
    "\t\t\t     num_classes=len(dog_dataset.classes),\n",
    "\t\t\t     kernel_size=7,\n",
    "\t\t\t     stride = 2,\n",
    "\t\t\t     dropout = None,\n",
    "\t\t\t     activation = nn.ReLU(inplace = False))\n",
    "model.to(device)\n",
    "\n",
    "def train_valid(optimizer = optim.Adam(model.parameters()), epochs = 20, model = model,\n",
    "                train_criterion = nn.CrossEntropyLoss(), train_loader = train_loader,\n",
    "                valid_criterion = nn.CrossEntropyLoss(), valid_loader = valid_loader,\n",
    "                device = device):\n",
    "\n",
    "    start_epoch = 1\n",
    "    #or: best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[],\n",
    "                \"valid_loss\":[], \"valid_acc\":[], \"valid_preds_list\":[],\n",
    "                \"valid_truelabels_list\":[], \"valid_probas_list\":[], \"valid_auc_score\":[]}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, optimizer, train_criterion, train_loader, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('{}: loss: {:.4f} acc: {:.4f}'.format('training', train_loss, train_acc))\n",
    "\n",
    "        valid_loss, valid_acc, valid_preds_list, valid_truelabels_list, valid_probas_list, valid_auc_score = validation(epoch, model, optimizer, valid_criterion, valid_loader, device)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"valid_acc\"].append(valid_acc)\n",
    "        history[\"valid_preds_list\"].append(valid_preds_list)\n",
    "        history[\"valid_truelabels_list\"].append(valid_truelabels_list)\n",
    "        history[\"valid_probas_list\"].append(valid_probas_list)\n",
    "        history[\"valid_auc_score\"].append(valid_auc_score)\n",
    "\n",
    "        print('{}: loss: {:.4f} acc: {:.4f} auc: {:.4f}'.format('validation', valid_loss, valid_acc, valid_auc_score))\n",
    "        print()\n",
    "\n",
    "        # save models(use valid loss as best model criterion, please change\n",
    "        # criterion here if needed(eg. valid acc)\n",
    "        is_best = valid_loss < best_val_loss\n",
    "        best_val_loss = min(valid_loss, best_val_loss)\n",
    "\n",
    "        if is_best:\n",
    "            # please change model file path here\n",
    "            best_model_file = \"best_models/best_dry_run1.pth\"\n",
    "            torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "        # save model from every training epoch\n",
    "        # can be deleted if do not need this one, or adapt it to save 5th, 10th, 15th ...models\n",
    "        model_file = \"best_models/dry_run1\" + str(epoch) + \".pth\"\n",
    "\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "\n",
    "        # save training/validation results\n",
    "        with open(\"history.pkl\", \"wb\") as fout:\n",
    "            pickle.dump(history, fout)\n",
    "\n",
    "    print('time elapsed:', time.time() - start_time)\n",
    "\n",
    "    return history\n",
    "\n",
    "results = train_valid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "############## Test ###############\n",
    "###################################\n",
    "print('###################################')\n",
    "print('############# Test ################')\n",
    "print('###################################')\n",
    "\n",
    "def test(optimizer = optim.Adam(model.parameters()), model = model,  test_criterion = nn.CrossEntropyLoss(),\n",
    "         loader = test_loader, device = device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    mysoftmax = nn.Softmax(dim=1)\n",
    "\n",
    "    preds_list = []\n",
    "    truelabels_list = []\n",
    "    probas_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(loader):\n",
    "\n",
    "            image = samples['image'].to(device)\n",
    "            label = samples['label'].squeeze()\n",
    "            label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "\n",
    "            output = model(image)\n",
    "            output_softmax = mysoftmax(output)\n",
    "\n",
    "            _, preds = torch.max(output, dim = 1)\n",
    "\n",
    "            loss = test_criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            total_samples += image.shape[0]\n",
    "            correct += torch.sum(preds == label).item()\n",
    "\n",
    "\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "            truelabels_list.append(label.cpu().numpy())\n",
    "            probas_list.append(output_softmax.cpu().numpy())\n",
    "\n",
    "        test_accuracy = correct / total_samples\n",
    "\n",
    "        return running_loss / len(loader), test_accuracy, preds_list, truelabels_list, probas_list\n",
    "\n",
    "\n",
    "test_loss, test_acc,  preds_list, truelabels_list, probas_list= test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
