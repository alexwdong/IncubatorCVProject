{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############# Notice ##############\n",
    "###################################\n",
    "\n",
    "# Some parts need to be adapted later\n",
    "\n",
    "###################################\n",
    "########## import library #########\n",
    "###################################\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor,Resize,Compose\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.train_valid import train, validation\n",
    "from src.Basic_CNN_Architecture import BasicCNN_128x128\n",
    "from src.data_loader import load_dog_data,SquarePadding\n",
    "from src.utils import unravel_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "########### load device ###########\n",
    "###################################\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():\n",
    "\n",
    "    # Tell PyTorch to use the GPU.\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################\n",
    "############ def path #############\n",
    "###################################\n",
    "\n",
    "# hpc\n",
    "# please change file path here\n",
    "        \n",
    "\n",
    "#val_df_path =\n",
    "#test_df_path =\n",
    "#root_dir =\n",
    "\n",
    "\n",
    "\n",
    "#Put your data path here\n",
    "data_path = r'C:\\Users\\Alexander Dong\\IncubatorCVProject\\DogsDataset\\images\\Images'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############ load data ############\n",
    "###################################\n",
    "\n",
    "# after feature engineering(task 3)\n",
    "\n",
    "#Variables for splitting the dataset into train/test\n",
    "valid_split = .1\n",
    "test_split = .1\n",
    "batch_size = 16\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "#Load initial dataset\n",
    "dog_dataset = ImageFolder(data_path,transform=Compose([\n",
    "    SquarePadding(),\n",
    "    Resize((128,128)),\n",
    "    ToTensor()\n",
    "])) \n",
    "# Split \n",
    "dataset_size = len(dog_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split_idx1 = int(np.floor((valid_split+test_split) * dataset_size))\n",
    "split_idx2 = int(np.floor(test_split * dataset_size))\n",
    "if shuffle_dataset:\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "test_indices, valid_indices, train_indices = indices[:split_idx2], indices[split_idx2:split_idx1], indices[split_idx1:]\n",
    "\n",
    "# Creating PT data samplers and loaders:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(valid_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "#Load Train, valid, test          \n",
    "train_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "test_loader =  torch.utils.data.DataLoader(dog_dataset, batch_size=batch_size,\n",
    "                                                sampler=test_sampler)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############ Train ############\n",
    "###################################\n",
    "print('###################################')\n",
    "print('############## Train ##############')\n",
    "print('###################################')\n",
    "\n",
    "# training process\n",
    "# to be finished later\n",
    "model = BasicCNN_128x128(num_classes=len(dog_dataset.classes),\n",
    "                        )\n",
    "model.to(device)\n",
    "\n",
    "def train_valid(optimizer = optim.Adam(model.parameters()), epochs = 20, model = model,\n",
    "                train_criterion = nn.CrossEntropyLoss(), train_loader = train_loader,\n",
    "                valid_criterion = nn.CrossEntropyLoss(), valid_loader = valid_loader,\n",
    "                device = device):\n",
    "\n",
    "    start_epoch = 1\n",
    "    #or: best_val_acc = 0\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    history = {\"train_loss\":[], \"train_acc\":[],\n",
    "                \"valid_loss\":[], \"valid_acc\":[], \"valid_preds_list\":[],\n",
    "                \"valid_truelabels_list\":[], \"valid_probas_list\":[], \"valid_auc_score\":[]}\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs + 1):\n",
    "\n",
    "        train_loss, train_acc = train(epoch, model, optimizer, train_criterion, train_loader, device)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "\n",
    "        print('epoch: ', epoch)\n",
    "        print('{}: loss: {:.4f} acc: {:.4f}'.format('training', train_loss, train_acc))\n",
    "\n",
    "        valid_loss, valid_acc, valid_preds_list, valid_truelabels_list, valid_probas_list, valid_auc_score = validation(epoch, model, optimizer, valid_criterion, valid_loader, device)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "        history[\"valid_acc\"].append(valid_acc)\n",
    "        history[\"valid_preds_list\"].append(valid_preds_list)\n",
    "        history[\"valid_truelabels_list\"].append(valid_truelabels_list)\n",
    "        history[\"valid_probas_list\"].append(valid_probas_list)\n",
    "        history[\"valid_auc_score\"].append(valid_auc_score)\n",
    "\n",
    "        print('{}: loss: {:.4f} acc: {:.4f} auc: {:.4f}'.format('validation', valid_loss, valid_acc, valid_auc_score))\n",
    "        print()\n",
    "\n",
    "        # save models(use valid loss as best model criterion, please change\n",
    "        # criterion here if needed(eg. valid acc)\n",
    "        is_best = valid_loss < best_val_loss\n",
    "        best_val_loss = min(valid_loss, best_val_loss)\n",
    "\n",
    "        if is_best:\n",
    "            # please change model file path here\n",
    "            best_model_file = \"best_models/best_dry_run1.pth\"\n",
    "            torch.save(model.state_dict(), best_model_file)\n",
    "\n",
    "        # save model from every training epoch\n",
    "        # can be deleted if do not need this one, or adapt it to save 5th, 10th, 15th ...models\n",
    "        model_file = \"best_models/dry_run1\" + str(epoch) + \".pth\"\n",
    "\n",
    "        torch.save(model.state_dict(), model_file)\n",
    "\n",
    "        # save training/validation results\n",
    "        with open(\"history.pkl\", \"wb\") as fout:\n",
    "            pickle.dump(history, fout)\n",
    "\n",
    "    print('time elapsed:', time.time() - start_time)\n",
    "\n",
    "    return history\n",
    "\n",
    "results = train_valid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load eigenvalues and eigenvectors\n",
    "\n",
    "(eig_vals,eig_image_list) = pickle.load( open( \"eigvalsvecs.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn eigenvectors from list into matrix\n",
    "eig_vec_mat = np.zeros((eig_vec_list[0].shape[0]*(eig_vec_list[0].shape[1]),len(eig_vec_list)))\n",
    "for ii,vec in enumerate(eig_vec_list):\n",
    "    eig_vec_mat[:,ii] = unravel_image(vec)\n",
    "  \n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4096, 4096)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vec_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############## Test ###############\n",
    "###################################\n",
    "print('###################################')\n",
    "print('############# Test ################')\n",
    "print('###################################')\n",
    "\n",
    "def test(optimizer = optim.Adam(model.parameters()), model = model,  test_criterion = nn.CrossEntropyLoss(),\n",
    "         loader = test_loader, device = device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    mysoftmax = nn.Softmax(dim=1)\n",
    "\n",
    "    preds_list = []\n",
    "    truelabels_list = []\n",
    "    probas_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, samples in enumerate(loader):\n",
    "\n",
    "            image = samples['image'].to(device)\n",
    "            label = samples['label'].squeeze()\n",
    "            label = torch.tensor(label, dtype=torch.long, device=device)\n",
    "\n",
    "            output = model(image)\n",
    "            output_softmax = mysoftmax(output)\n",
    "\n",
    "            _, preds = torch.max(output, dim = 1)\n",
    "\n",
    "            loss = test_criterion(output, label)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            total_samples += image.shape[0]\n",
    "            correct += torch.sum(preds == label).item()\n",
    "\n",
    "\n",
    "            preds_list.append(preds.cpu().numpy())\n",
    "            truelabels_list.append(label.cpu().numpy())\n",
    "            probas_list.append(output_softmax.cpu().numpy())\n",
    "\n",
    "        test_accuracy = correct / total_samples\n",
    "\n",
    "        return running_loss / len(loader), test_accuracy, preds_list, truelabels_list, probas_list\n",
    "\n",
    "\n",
    "test_loss, test_acc,  preds_list, truelabels_list, probas_list= test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
